# Inference pada LLM

Inference adalah proses di mana model LLM (Large Language Model) yang telah dilatih menghasilkan respons berdasarkan input yang diberikan. Ini adalah tahap di mana LLM benar-benar "berpikir" dan menghasilkan output yang kita lihat.

## Pengertian Inference

Inference adalah proses komputasi di mana model yang telah dilatih sebelumnya diterapkan untuk menghasilkan prediksi atau respon terhadap input baru yang belum pernah dilihat sebelumnya oleh model. Dalam konteks LLM, inference melibatkan penggunaan model untuk:

1. Memahami konteks input
2. Memproses informasi sesuai dengan pengetahuan yang diperoleh selama pelatihan
3. Menghasilkan output berupa teks yang relevan dan koheren

## Tahapan Proses Inference

Proses inference pada LLM meliputi beberapa tahapan utama:

### 1. Tokenisasi Input

- Input teks dipecah menjadi unit-unit yang lebih kecil yang disebut token
- Token bisa berupa kata utuh, bagian kata, atau karakter tunggal
- Setiap model memiliki tokenizer khusus yang dikembangkan bersamaan dengan model

### 2. Encoding

- Token-token tersebut dikonversi menjadi vektor numerik (embedding)
- Embedding ini mewakili representasi semantik dari token dalam ruang vektor multi-dimensi

### 3. Propagasi Melalui Model

- Embedding input dilewatkan melalui arsitektur neural network model
- Model memproses input melalui layers yang terdiri dari self-attention dan feed-forward networks
- Informasi kontekstual dipertahankan dan diproses secara mendalam

### 4. Prediksi Token Berikutnya

- Model menghitung distribusi probabilitas untuk token yang mungkin muncul berikutnya
- Probabilitas ini didasarkan pada pola yang telah dipelajari model selama pelatihan

### 5. Sampling dan Decoding

- Token berikutnya dipilih berdasarkan distribusi probabilitas
- Berbagai strategi sampling dapat digunakan:
  - **Greedy Decoding**: Selalu memilih token dengan probabilitas tertinggi
  - **Top-k Sampling**: Memilih secara acak dari k token dengan probabilitas tertinggi
  - **Top-p (Nucleus) Sampling**: Memilih dari subset token yang probabilitasnya berjumlah p
  - **Temperature Sampling**: Mengontrol "kreativitas" output dengan parameter suhu

### 6. Pengulangan Proses

- Token yang baru dipilih ditambahkan ke konteks input
- Proses diulang hingga kondisi penghentian tercapai (batas token tercapai, token khusus penghentian dihasilkan, dll.)

### 7. Post-processing

- Output token dikonversi kembali menjadi teks yang dapat dibaca manusia
- Format output dapat disesuaikan sesuai kebutuhan aplikasi

## Parameter Inference yang Dapat Diatur

### 1. Temperature

- Mengontrol tingkat keacakan (randomness) dalam output
- Nilai rendah (0.1 - 0.4): Output lebih deterministik dan fokus
- Nilai tinggi (0.7 - 1.0): Output lebih kreatif dan beragam
- Nilai 0: Selalu memilih token dengan probabilitas tertinggi (greedy decoding)

### 2. Top-p (Nucleus Sampling)

- Membatasi pemilihan token pada subset dengan probabilitas kumulatif mencapai nilai p
- Nilai yang umum: 0.9 - 0.95
- Membantu menjaga keseimbangan antara kreativitas dan koherensi

### 3. Top-k

- Membatasi pemilihan token hanya pada k token dengan probabilitas tertinggi
- Membantu menghindari pemilihan token yang sangat tidak mungkin

### 4. Frequency Penalty

- Mengurangi kemungkinan model mengulang token yang sudah sering muncul
- Membantu menghindari pengulangan dan loop

### 5. Presence Penalty

- Mengurangi kemungkinan model mengulang token yang sudah pernah muncul
- Mendorong keberagaman konten

### 6. Max Tokens

- Batas maksimum jumlah token yang akan dihasilkan
- Penting untuk mengontrol panjang output dan biaya

## Optimasi Inference

### 1. Kuantisasi

- Mengurangi presisi numerik parameter model (misalnya dari FP32 ke INT8)
- Mengurangi kebutuhan memori dan meningkatkan kecepatan dengan sedikit penurunan kualitas

### 2. Knowledge Distillation

- Melatih model yang lebih kecil untuk meniru perilaku model yang lebih besar
- Menghasilkan model yang lebih cepat dengan kualitas yang masih dapat diterima

### 3. Paralelisasi

- Membagi komputasi di beberapa perangkat keras
- Model paralelisme: Membagi layer model ke beberapa GPU
- Tensor paralelisme: Membagi operasi tensor individual ke beberapa GPU

### 4. Caching

- Menyimpan state hidden dari token yang telah diproses
- Menghindari perhitungan ulang untuk token yang sama dalam konteks yang sama

## Metrik Evaluasi Inference

### 1. Latency (Latensi)

- Waktu yang dibutuhkan untuk menghasilkan respons
- Biasanya diukur dalam milidetik atau detik
- Penting untuk aplikasi real-time

### 2. Throughput

- Jumlah token atau permintaan yang dapat diproses per unit waktu
- Diukur dalam token per detik atau permintaan per detik
- Penting untuk sistem dengan beban tinggi

### 3. Efisiensi Memori

- Penggunaan RAM selama inference
- Menentukan kapasitas server yang dibutuhkan

### 4. Biaya Komputasi

- Biaya untuk melakukan inference (terutama pada layanan cloud)
- Biasanya dihitung per 1000 token

## Tantangan dalam Inference LLM

### 1. Halusinasi

- Model menghasilkan informasi yang tidak akurat atau tidak didukung fakta
- Dapat diminimalisir dengan teknik RAG (Retrieval-Augmented Generation)

### 2. Batas Konteks

- Model memiliki batas jumlah token yang dapat diproses sekaligus
- Memerlukan strategi khusus untuk penanganan dokumen panjang

### 3. Latensi vs Kualitas

- Trade-off antara kecepatan respons dan kualitas output
- Model yang lebih besar umumnya menghasilkan output lebih baik tetapi lebih lambat

### 4. Bias dan Keamanan

- Model dapat mereplikasi atau memperkuat bias dari data pelatihan
- Memerlukan filter keamanan tambahan untuk menghindari output berbahaya

## Implementasi Praktis Inference

### 1. On-premise vs Cloud

- **On-premise**: Kontrol penuh, keamanan data, biaya awal tinggi
- **Cloud**: Skalabilitas, biaya operasional, kemudahan pengelolaan

### 2. Batching

- Menggabungkan beberapa permintaan untuk diproses sekaligus
- Meningkatkan throughput tetapi dapat meningkatkan latensi

### 3. Streaming

- Mengirimkan token output secepat mereka dihasilkan
- Meningkatkan pengalaman pengguna untuk respons panjang

## Kesimpulan

Inference adalah jantung dari interaksi dengan LLM. Memahami proses inference dan parameter yang dapat diatur memungkinkan pengembang untuk mengoptimalkan performa model dan menghasilkan output yang sesuai dengan kebutuhan aplikasi. Dengan perkembangan teknologi, proses inference terus ditingkatkan untuk menjadi lebih cepat, lebih efisien, dan menghasilkan output yang lebih berkualitas.